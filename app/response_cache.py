"""
Response Cache - √âvite g√©n√©ration de r√©ponses identiques/similaires

Utilise Redis pour stocker:
1. Intent de r√©ponse (flag "generating")
2. R√©ponses r√©centes (pour deduplication)
3. TTL 60s (expire automatiquement)
"""

import json
import hashlib
import re
from datetime import datetime
from typing import Optional, List, Dict, Set
import redis.asyncio as redis
from difflib import SequenceMatcher
import logging

logger = logging.getLogger(__name__)


class ResponseCache:
    """Cache Redis pour √©viter doublons de r√©ponses"""
    
    def __init__(self, redis_client: redis.Redis):
        self.redis = redis_client
        self.CACHE_TTL = 60  # Secondes - expire auto
        
        # Seuils de d√©tection (multi-niveaux)
        self.TEXT_SIMILARITY_THRESHOLD = 0.75  # 75% similarit√© textuelle
        self.KEYWORD_OVERLAP_THRESHOLD = 0.70  # 70% mots-cl√©s communs
        self.START_SIMILARITY_THRESHOLD = 0.85 # 85% d√©but identique
    
    def _make_key(self, match_id: str, key_type: str) -> str:
        """G√©n√®re cl√© Redis"""
        return f"response:{key_type}:{match_id}"
    
    def _normalize_text(self, text: str) -> str:
        """Normalise un texte pour comparaison"""
        # Lowercase
        text = text.lower()
        
        # Enlever ponctuation (garder lettres, chiffres, espaces)
        text = re.sub(r'[^a-z0-9√†√¢√§√©√®√™√´√Ø√Æ√¥√π√ª√º√ø≈ì√¶√ß\s]', '', text)
        
        # Normaliser espaces
        text = ' '.join(text.split())
        
        return text
    
    def _extract_keywords(self, text: str) -> Set[str]:
        """Extrait mots-cl√©s importants (>3 caract√®res, pas stop words)"""
        # Stop words fran√ßais communs
        stop_words = {
            'le', 'la', 'les', 'un', 'une', 'des', 'de', 'du',
            'et', 'ou', 'mais', 'donc', 'car', 'ni',
            'je', 'tu', 'il', 'elle', 'nous', 'vous', 'ils', 'elles',
            'mon', 'ton', 'son', 'ma', 'ta', 'sa', 'mes', 'tes', 'ses',
            'ce', 'cette', 'ces', 'cet',
            'qui', 'que', 'quoi', 'dont', 'o√π',
            'sur', 'sous', 'dans', 'avec', 'sans', 'pour', 'par',
            'au', 'aux', '√†', 'en',
            'est', 'sont', '√©tait', '√©t√©', '√™tre', 'avoir', 'avait', 'eu',
            'fait', 'faire', 'fais', 'faisait',
            'pas', 'plus', 'tr√®s', 'bien', 'tout', 'tous', 'toute',
            'comme', 'comment', 'quand', 'pourquoi',
            'toi', 'moi', 'lui',
            'oui', 'non', 'peut', 'peu', 'beaucoup',
            '√ßa', 'cela', '√ßa va', 'va', 'vais', 'vas',
        }
        
        normalized = self._normalize_text(text)
        words = normalized.split()
        
        # Garder mots >3 chars, pas stop words
        keywords = {
            word for word in words 
            if len(word) > 3 and word not in stop_words
        }
        
        return keywords
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """Calcule similarit√© textuelle pure (SequenceMatcher)"""
        norm1 = self._normalize_text(text1)
        norm2 = self._normalize_text(text2)
        return SequenceMatcher(None, norm1, norm2).ratio()
    
    def _calculate_keyword_overlap(self, text1: str, text2: str) -> float:
        """Calcule overlap des mots-cl√©s (Jaccard similarity)"""
        keywords1 = self._extract_keywords(text1)
        keywords2 = self._extract_keywords(text2)
        
        if not keywords1 or not keywords2:
            return 0.0
        
        # Jaccard similarity: intersection / union
        intersection = keywords1 & keywords2
        union = keywords1 | keywords2
        
        return len(intersection) / len(union) if union else 0.0
    
    def _calculate_start_similarity(self, text1: str, text2: str) -> float:
        """Calcule similarit√© des d√©buts de phrases"""
        norm1 = self._normalize_text(text1)
        norm2 = self._normalize_text(text2)
        
        # Prendre les 50 premiers caract√®res ou moins
        length = min(50, len(norm1), len(norm2))
        if length < 10:  # Trop court pour comparer
            return 0.0
        
        start1 = norm1[:length]
        start2 = norm2[:length]
        
        return SequenceMatcher(None, start1, start2).ratio()
    
    def _are_responses_similar(self, response1: str, response2: str) -> tuple[bool, str]:
        """
        D√©tection multi-niveaux de similarit√©
        
        Returns:
            (is_similar, reason)
        """
        # Niveau 1 : Similarit√© textuelle pure
        text_sim = self._calculate_text_similarity(response1, response2)
        if text_sim >= self.TEXT_SIMILARITY_THRESHOLD:
            return True, f"Similarit√© textuelle: {text_sim:.2%}"
        
        # Niveau 2 : Overlap des mots-cl√©s
        keyword_overlap = self._calculate_keyword_overlap(response1, response2)
        if keyword_overlap >= self.KEYWORD_OVERLAP_THRESHOLD:
            keywords1 = self._extract_keywords(response1)
            keywords2 = self._extract_keywords(response2)
            common = keywords1 & keywords2
            return True, f"Mots-cl√©s communs ({keyword_overlap:.2%}): {common}"
        
        # Niveau 3 : D√©but identique + longueur similaire
        start_sim = self._calculate_start_similarity(response1, response2)
        length_ratio = min(len(response1), len(response2)) / max(len(response1), len(response2))
        
        if start_sim >= self.START_SIMILARITY_THRESHOLD and length_ratio >= 0.7:
            return True, f"D√©but identique ({start_sim:.2%}) + longueur similaire ({length_ratio:.2%})"
        
        return False, "R√©ponses suffisamment diff√©rentes"
    
    async def is_generating(self, match_id: str) -> bool:
        """Check si une g√©n√©ration est d√©j√† en cours"""
        key = self._make_key(match_id, "generating")
        exists = await self.redis.exists(key)
        
        if exists:
            data = await self.redis.get(key)
            if data:
                generating_data = json.loads(data)
                logger.info(f"‚ö†Ô∏è G√©n√©ration d√©j√† en cours depuis {generating_data.get('started_at')}")
                return True
        
        return False
    
    async def mark_generating(self, match_id: str, user_message: str):
        """Marque qu'une g√©n√©ration d√©marre"""
        key = self._make_key(match_id, "generating")
        
        data = {
            'started_at': datetime.now().isoformat(),
            'user_message': user_message,
            'status': 'generating'
        }
        
        await self.redis.setex(
            key,
            self.CACHE_TTL,
            json.dumps(data)
        )
        
        logger.info(f"üîí G√©n√©ration marqu√©e en cours (TTL {self.CACHE_TTL}s)")
    
    async def clear_generating(self, match_id: str):
        """Clear le flag de g√©n√©ration"""
        key = self._make_key(match_id, "generating")
        await self.redis.delete(key)
        logger.info("üîì Flag g√©n√©ration cleared")
    
    async def store_response(self, match_id: str, bot_response: str, user_message: str):
        """Stocke une r√©ponse dans le cache"""
        key = self._make_key(match_id, "recent")
        
        # R√©cup√©rer r√©ponses existantes
        existing = await self.redis.get(key)
        
        if existing:
            responses = json.loads(existing)
        else:
            responses = []
        
        # Ajouter nouvelle r√©ponse
        responses.append({
            'response': bot_response,
            'user_message': user_message,
            'timestamp': datetime.now().isoformat()
        })
        
        # Garder seulement les 5 derni√®res
        responses = responses[-5:]
        
        # Stocker avec TTL
        await self.redis.setex(
            key,
            self.CACHE_TTL,
            json.dumps(responses)
        )
        
        logger.info(f"üíæ R√©ponse stock√©e en cache (total: {len(responses)})")
    
    async def find_similar_response(
        self, 
        match_id: str, 
        user_message: str
    ) -> Optional[str]:
        """
        Cherche une r√©ponse similaire r√©cente
        
        Returns:
            str si doublon trouv√©, None sinon
        """
        key = self._make_key(match_id, "recent")
        existing = await self.redis.get(key)
        
        if not existing:
            return None
        
        responses = json.loads(existing)
        
        # Check chaque r√©ponse r√©cente
        for resp_data in responses:
            # Comparer user_message pour voir si m√™me contexte
            user_similarity = self._calculate_text_similarity(
                user_message,
                resp_data['user_message']
            )
            
            if user_similarity > 0.7:  # M√™me question
                logger.warning(f"‚ö†Ô∏è Question similaire trouv√©e!")
                logger.warning(f"   User msg: {user_message[:50]}")
                logger.warning(f"   Cached: {resp_data['user_message'][:50]}")
                logger.warning(f"   Similarity: {user_similarity:.2%}")
                logger.warning(f"   ‚Üí R√©ponse d√©j√† envoy√©e: {resp_data['response'][:50]}")
                
                return resp_data['response']
        
        return None
    
    async def is_duplicate_response(
        self,
        match_id: str,
        new_response: str
    ) -> bool:
        """
        Check si la r√©ponse g√©n√©r√©e est trop similaire aux r√©centes
        Utilise d√©tection multi-niveaux
        
        Returns:
            True si doublon, False sinon
        """
        key = self._make_key(match_id, "recent")
        existing = await self.redis.get(key)
        
        if not existing:
            return False
        
        responses = json.loads(existing)
        
        # Comparer avec chaque r√©ponse r√©cente
        for resp_data in responses:
            is_similar, reason = self._are_responses_similar(
                new_response,
                resp_data['response']
            )
            
            if is_similar:
                logger.warning(f"‚ùå DOUBLON D√âTECT√â!")
                logger.warning(f"   Nouvelle: {new_response[:60]}")
                logger.warning(f"   Existante: {resp_data['response'][:60]}")
                logger.warning(f"   Raison: {reason}")
                logger.warning(f"   ‚Üí SKIP cette r√©ponse")
                
                return True
        
        # Toutes les comparaisons ont √©chou√© ‚Üí Pas de doublon
        logger.info("‚úÖ R√©ponse suffisamment diff√©rente des r√©centes")
        return False
    
    async def get_recent_responses(self, match_id: str) -> List[Dict]:
        """R√©cup√®re toutes les r√©ponses r√©centes (debug)"""
        key = self._make_key(match_id, "recent")
        existing = await self.redis.get(key)
        
        if existing:
            return json.loads(existing)
        
        return []
