"""
Bridge intelligent avec grouping des messages rapides
"""

import asyncio
import asyncpg
import json
import redis.asyncio as redis
from datetime import datetime
from typing import Dict
from app.config import settings
from app.redis_context import RedisContextManager
from app.match_monitor import MatchMonitor
from app.supabase_client import SupabaseClient
import logging

logger = logging.getLogger(__name__)


class BridgeIntelligence:
    """Bridge avec grouping intelligent + cooldown anti-duplication"""
    
    def __init__(self):
        self.pg_conn = None
        self.redis_client = None
        self.context_manager = None
        self.match_monitor = None
        self.supabase_client = None
        self.running = False
        self.GROUPING_DELAY = 15  # Secondes (laisser temps √† user de finir)
        
        # üÜï COOLDOWN SYSTEM - Anti-duplication
        self.last_push_times: Dict[str, datetime] = {}  # {match_id: datetime}
        self.PUSH_COOLDOWN = 5  # Secondes - √©vite cr√©ation jobs multiples
        
    async def connect_postgres(self):
        """Connexion PostgreSQL"""
        logger.info("üîå Connexion √† PostgreSQL...")
        self.pg_conn = await asyncpg.connect(
            settings.postgres_connection_string,
            server_settings={'jit': 'off'}
        )
        logger.info("‚úÖ Connect√© √† PostgreSQL")
        
    async def connect_redis(self):
        """Connexion Redis"""
        logger.info("üîå Connexion √† Redis...")
        self.redis_client = await redis.from_url(
            settings.redis_url,
            encoding="utf-8",
            decode_responses=True
        )
        self.context_manager = RedisContextManager(self.redis_client)
        
        # Cr√©er SupabaseClient pour match_monitor
        self.supabase_client = SupabaseClient()
        await self.supabase_client.connect()
        
        self.match_monitor = MatchMonitor(self.supabase_client)
        logger.info("‚úÖ Connect√© √† Redis")
    
    async def push_to_queue(self, message: Dict, max_retries: int = 3):
        """Pousse un message dans la queue avec retry automatique"""
        queue_key = "bot_messages"
        
        for attempt in range(max_retries):
            try:
                await self.redis_client.rpush(queue_key, json.dumps(message))
                logger.info(f"‚úÖ Message pouss√© dans queue")
                return  # Succ√®s
                
            except redis.ConnectionError as e:
                logger.warning(f"‚ö†Ô∏è Tentative {attempt + 1}/{max_retries} - Connexion Redis perdue: {e}")
                
                if attempt < max_retries - 1:
                    # Attendre avant retry (backoff exponentiel)
                    wait_time = 2 ** attempt  # 1s, 2s, 4s
                    logger.info(f"‚è≥ Retry dans {wait_time}s...")
                    await asyncio.sleep(wait_time)
                    
                    # Tenter reconnexion Redis
                    try:
                        logger.info("üîÑ Reconnexion Redis...")
                        await self.redis_client.close()
                        self.redis_client = await redis.from_url(
                            settings.redis_url,
                            encoding="utf-8",
                            decode_responses=True
                        )
                        self.context_manager = RedisContextManager(self.redis_client)
                        logger.info("‚úÖ Reconnexion Redis r√©ussie")
                    except Exception as reconnect_error:
                        logger.error(f"‚ùå Reconnexion Redis √©chou√©e: {reconnect_error}")
                else:
                    # Derni√®re tentative √©chou√©e
                    logger.error(f"‚ùå √âchec d√©finitif apr√®s {max_retries} tentatives")
                    raise
                    
            except Exception as e:
                logger.error(f"‚ùå Erreur inattendue push_to_queue: {type(e).__name__}: {e}")
                raise
    
    async def delayed_push(self, match_id: str):
        """
        Attendre X secondes puis pousser les messages group√©s
        """
        await asyncio.sleep(self.GROUPING_DELAY)
        
        context = await self.context_manager.get_context(match_id)
        
        if context and len(context['messages']) > 0:
            # Cr√©er payload group√©
            grouped = {
                'type': 'grouped',
                'count': context['rapid_count'],
                'messages': context['messages'],
                'match_id': match_id,
                'bot_id': context['messages'][0]['bot_id']
            }
            
            logger.info(f"üì¶ Grouping: {context['rapid_count']} messages")
            await self.push_to_queue(grouped)
            
            # üÜï ENREGISTRER TEMPS DU PUSH - Active cooldown
            self.last_push_times[match_id] = datetime.now()
            logger.info(f"‚è∞ Cooldown activ√© pour {self.PUSH_COOLDOWN}s")
            
            # Nettoyer contexte
            await self.context_manager.delete_context(match_id)
    
    async def handle_notification(self, connection, pid, channel, payload):
        """
        Callback PostgreSQL NOTIFY avec grouping intelligent + cooldown
        """
        try:
            logger.info(f"üì® Notification re√ßue (pid={pid}, channel={channel})")
            logger.debug(f"   Payload: {payload[:200]}...")  # Log premiers 200 chars
            
            message = json.loads(payload)
            match_id = message['match_id']
            logger.debug(f"   Match ID: {match_id}")
            
            # üÜï CHECK COOLDOWN - √âvite jobs multiples pour messages rapproch√©s
            last_push = self.last_push_times.get(match_id)
            if last_push:
                time_since = (datetime.now() - last_push).total_seconds()
                if time_since < self.PUSH_COOLDOWN:
                    logger.info(f"‚è∏Ô∏è Cooldown actif ({time_since:.1f}s), ajout au prochain groupe")
                    # Ajouter au contexte sans cr√©er de timer
                    context = await self.context_manager.get_context(match_id)
                    if context:
                        await self.context_manager.update_context(match_id, message)
                        logger.info(f"   üìù Message ajout√© au contexte existant")
                    return  # Ne pas cr√©er de nouveau timer
            
            # R√©cup√©rer contexte existant
            context = await self.context_manager.get_context(match_id)
            
            if context:
                # Contexte existe = messages rapides
                last_time = datetime.fromisoformat(context['last_message_at'])
                time_diff = (datetime.now() - last_time).total_seconds()
                
                if time_diff < self.GROUPING_DELAY:
                    # Ajouter au grouping
                    logger.info(f"üîÑ Grouping: +1 message ({context['rapid_count'] + 1} total)")
                    await self.context_manager.update_context(match_id, message)
                    
                    # Si timer pas encore d√©marr√©, le d√©marrer maintenant
                    # NE PAS red√©marrer pour √©viter d√©lais infinis si user tape lentement
                    if not context.get('timer_started'):
                        logger.info(f"‚è∞ D√©marrage timer grouping {self.GROUPING_DELAY}s")
                        asyncio.create_task(self.delayed_push(match_id))
                        context['timer_started'] = True
                        await self.context_manager.set_context(match_id, context)
                    else:
                        logger.info("   ‚è∞ Timer d√©j√† actif, pas de red√©marrage")
                    
                    return  # Ne pas pousser maintenant
            
            # Nouveau contexte : d√©marrer grouping (ne PAS pousser imm√©diatement)
            context = await self.context_manager.init_context(match_id, message)
            
            # D√©marrer timer pour voir si d'autres messages suivent
            logger.info(f"‚è∞ Nouveau message, d√©marrage timer {self.GROUPING_DELAY}s")
            asyncio.create_task(self.delayed_push(match_id))
            
            # Marquer timer comme d√©marr√©
            context['timer_started'] = True
            await self.context_manager.set_context(match_id, context)
            
        except json.JSONDecodeError as e:
            logger.error(f"‚ùå Erreur parsing JSON payload: {e}")
            logger.error(f"   Payload re√ßu: {payload}")
        except KeyError as e:
            logger.error(f"‚ùå Cl√© manquante dans payload: {e}")
            logger.error(f"   Message: {message}")
        except Exception as e:
            logger.error(f"‚ùå Erreur inattendue dans handle_notification: {type(e).__name__}: {e}")
            logger.exception("Stack trace complet:")  # Log full stack trace
    
    async def handle_new_match(self, connection, pid, channel, payload):
        """
        Callback PostgreSQL NOTIFY pour nouveaux matchs
        """
        try:
            logger.info(f"üîç Nouveau match d√©tect√©")
            
            match_data = json.loads(payload)
            
            # Passer le Dict complet √† match_monitor
            # Il identifiera lui-m√™me qui est le bot
            await self.match_monitor.process_new_match(match_data)
            
        except Exception as e:
            logger.error(f"‚ùå Erreur nouveau match: {e}")
    
    async def start_listening(self):
        """D√©marre l'√©coute PostgreSQL"""
        logger.info("üëÇ D√©marrage √©coute 'bot_events' et 'new_match'...")
        
        await self.pg_conn.add_listener('bot_events', self.handle_notification)
        await self.pg_conn.add_listener('new_match', self.handle_new_match)
        
        logger.info("‚úÖ √âcoute active (messages + nouveaux matchs)")
        logger.info("‚è≥ En attente...")
        
        self.running = True
        
        # Compteur pour keepalive
        keepalive_counter = 0
        
        try:
            while self.running:
                await asyncio.sleep(1)
                
                # Keepalive toutes les 30 secondes
                keepalive_counter += 1
                if keepalive_counter >= 30:
                    try:
                        # Simple query pour garder connexion active
                        await self.pg_conn.fetchval('SELECT 1')
                        logger.debug("üíì Keepalive PostgreSQL")
                        keepalive_counter = 0
                    except Exception as e:
                        logger.error(f"‚ùå Keepalive √©chou√©, reconnexion...")
                        # Reconnexion
                        await self.reconnect()
                        keepalive_counter = 0
                        
        except KeyboardInterrupt:
            await self.stop()
    
    async def reconnect(self):
        """Reconnecte au PostgreSQL si connexion perdue"""
        try:
            logger.warning("üîÑ Reconnexion PostgreSQL...")
            
            # Fermer ancienne connexion
            if self.pg_conn:
                await self.pg_conn.close()
            
            # Nouvelle connexion
            await self.connect_postgres()
            
            # Re-setup listeners
            await self.pg_conn.add_listener('bot_events', self.handle_notification)
            await self.pg_conn.add_listener('new_match', self.handle_new_match)
            
            logger.info("‚úÖ Reconnexion r√©ussie")
            
        except Exception as e:
            logger.error(f"‚ùå Reconnexion √©chou√©e: {e}")
            # Retry dans 5s
            await asyncio.sleep(5)
            await self.reconnect()
    
    async def stop(self):
        """Arr√™te le bridge"""
        logger.info("üõë Arr√™t bridge...")
        self.running = False
        
        if self.pg_conn:
            await self.pg_conn.remove_listener('bot_events', self.handle_notification)
            await self.pg_conn.remove_listener('new_match', self.handle_new_match)
            await self.pg_conn.close()
        
        if self.supabase_client:
            await self.supabase_client.close()
            
        if self.redis_client:
            await self.redis_client.close()
    
    async def run(self):
        """Lance le bridge"""
        logger.info("=" * 60)
        logger.info("üöÄ BRIDGE INTELLIGENCE - GROUPING ACTIF")
        logger.info("=" * 60)
        
        try:
            await self.connect_postgres()
            await self.connect_redis()
            await self.start_listening()
            
        except Exception as e:
            logger.error(f"‚ùå Erreur fatale: {e}")
            await self.stop()
            raise


async def main():
    # Configuration du logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(name)s - %(message)s'
    )
    
    bridge = BridgeIntelligence()
    await bridge.run()


if __name__ == "__main__":
    asyncio.run(main())
